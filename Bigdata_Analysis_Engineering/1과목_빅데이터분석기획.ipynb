{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e61e19",
   "metadata": {},
   "source": [
    "# 빅데이터의 이해 \n",
    "\n",
    "## 빅데이터 개요 및 활용 \n",
    "\n",
    "### 빅데이터 특징 \n",
    "\n",
    "1. 빅데이터의 개념  \n",
    "테라바이트 이상의 정형 및 비정형 데이터. 가치를 추출하고 결과를 분석하는 기술\n",
    "\n",
    "\n",
    "2. DIKW 피라미드  \n",
    "    - 데이터(Data) : 객관적 사실로서 다른 데이터와의 상관관계가 없는 가공하기 전의 순수한 수치\n",
    "    - 정보(Information) : 가공, 처리하여 데이터 간의 연관 관계와 함께 의미가 도출된 요소\n",
    "    - 지식(Knowledge) : 획득된 다양한 정보를 구조화하여 유의미한 정보를 분류하고 일반화시킨 결과\n",
    "    - 지혜(Wisdom) : 근본 원리에 대한 깊은 이해를 바탕으로 도출되는 창의적 아이디어\n",
    "\n",
    "\n",
    "3. 바이트의 크기 : KB-MB-GB-TB-PB-EB-ZB-YB\n",
    "\n",
    "\n",
    "4. 빅데이터의 특징\n",
    "    - 규모(Volumne) : 빅데이터 분석 규모에 관련된 특성, 디지털 정보량의 기하 급수적인 증가\n",
    "    - 다양성(Variety) : 빅데이터 자원 유형에 관련된 특성, 정형 + 반정형 + 비정형 데이터\n",
    "    - 속도(Velocity) : 빅데이터 수집, 분석, 활용 속도에 관련된 특성. 정보의 생성 속도 증가\n",
    "    - 신뢰성(Veracity) : 수집 대상 데이터가 가지는 신뢰에 관련된 특징\n",
    "    - 가치(Value) : 수집 데이터를 통해 얻을 수 있는 가치\n",
    "    - 정확성(Validity) : 수집 대상 데이터가 가지는 유효성과 정확성\n",
    "    - 휘발성(Volatility) : 수집 대상 데이터가 의미가 있는 기간\n",
    "\n",
    "\n",
    "5. 데이터 지식경영의 종류\n",
    "    - 지식구분\n",
    "        - 암묵지(Private) : 개인에게 체화되어 있는 지식\n",
    "        - 형식지(Public) : 문서나 메뉴얼처럼 형상화된 지식\n",
    "    - 지식 상호작용\n",
    "        - 내면화 : 형식지 -> 암묵지\n",
    "        - 공동화 : 암묵지 -> 암묵지\n",
    "        - 표출화 : 암묵지 -> 형식지\n",
    "        - 연결화 : 형식지 -> 형식지\n",
    "    \n",
    "### 빅데이터의 가치 \n",
    "\n",
    "1. 빅데이터의 가치 : 경제적 자산, 불확실성 제거, 리스크 감소, 스마트한 경쟁력, 타 분야와의 융합\n",
    "\n",
    "\n",
    "2. 빅데이터의 가치 산정이 어려운 이유\n",
    "    - 데이터 활용 방식의 다양화 : 언제/어디서/누가 활용할지 알 수 없어서\n",
    "    - 새로운 가치 창출 : 데이터가 기존에 없던 가치를 창출함\n",
    "    - 분석기술의 급속한 발전 : 비용 문제로 인해 분석할 수 없었던 것을 저렴한 비용으로 분석\n",
    "\n",
    "\n",
    "3. 빅데이터 위기 요인 및 통제 방안\n",
    "    - 위기요인\n",
    "        - 사생활 침해 : 개인정보가 포함된 데이터 노출\n",
    "        - 책임 원칙 훼손 : 분석 대상이 되는 사람들이 예측 알고리즘의 희생양이 됨\n",
    "        - 데이터 오용 : 에측의 한계, 잘못된 지표 사용\n",
    "    - 통제방안\n",
    "        - 사생활 침해 -> 책임의 강조\n",
    "        - 책임 원칙 훼손 -> 결과 기반의 책임 적용\n",
    "        - 데이터 오용 -> 알고리즘에 대한 접근 허용, 알고리즈미스트 전문가 필요\n",
    "\n",
    "\n",
    "4. 분석 가치 에스컬레이터\n",
    "    - 묘사분석 : 과거에 어떤 일이 일어났고, 현재는 무슨 일이 일어나고 있는지 확인\n",
    "    - 진단분석 : 분석의 원인을 이해하는 과정\n",
    "    - 예측분석 : 무슨일이 일어날 것인지를 예측\n",
    "    - 처방분석 : 예측을 바탕으로 최적화하는 과정\n",
    "    \n",
    "### 빅데이터 조직 및 인력 \n",
    "\n",
    "1. 빅데이터 업무 프로세스\n",
    "    - 도입단계 : 도입기획, 기술검토, 도입 조직 구성, 예산 확보\n",
    "    - 구축단계 : 요구사항 분석, 구현 테스트\n",
    "    - 운영단계 : 운영예산 고려\n",
    "\n",
    "\n",
    "2. 조직 구조 설계의 요소\n",
    "    - 업무활동 : 과업수행을 위해 수직업무 활동과 수평 업무 활동으로 구분\n",
    "        - 수직 업무 활동 : 경영계획, 예산 할당 등 우선순위를 결정\n",
    "        - 수평 업무 활동 : 업무 프로세스 절차별로 업무를 배분\n",
    "    - 부서화 : 조직구조 유형 설계\n",
    "        - 집중 구조 : 전사 분석 업무를 별도의 분석 전담 조직에서 수행\n",
    "        - 기능 구조 : 해당 부서에서 수행. 핵심 분석이 어려움\n",
    "        - 분산 구조 : 분석 결과에 따른 신속한 피드백이 나오고, 베스트 프랙티스 공유가 가능함\n",
    "    - 보고체계 : 조직의 목표 달성을 위하여 업무 활동 및 부서의 보고 체계 설계\n",
    "\n",
    "\n",
    "3. 조직 구조 설계 특성\n",
    "    - 공식화 : 기준을 사전에 설정하여 공식화\n",
    "    - 분업화 : 업무 수행시 업무를 분할하여 수행(수평분할, 수직분할)\n",
    "    - 직무 전문화\n",
    "    - 통제 범위 : 관리자가 효율적으로 관리할 수 있는 조직의 인원 수\n",
    "    - 의사소통 및 조정 : 지시, 보고, 피드백, 협업\n",
    "\n",
    "\n",
    "4. 데이터 사이언티스트 요구역량\n",
    "    - 소프트 스킬 : 커뮤니케이션 능력, 논리적 비판능력, 스토리텔링 능력, 시각화\n",
    "    - 하드 스킬 : 분석기술의 숙련도, 빅데이터 관련 이론적 지식\n",
    "    \n",
    "    \n",
    "5. 데이터 거버넌스 개념  \n",
    "데이터 거버넌스는 기업에서 사용하는 데이터의 가용성, 유용성, 통합성, 보완성을 관리하기 위한 정책과 프로세스를 다루며 프라이버시, 보안성, 데이터 품질, 관리 규정 준수를 강조하는 모델\n",
    "\n",
    "\n",
    "6. 데이터 거버넌스 구성요소\n",
    "    - 원칙 : 지침과 가이드\n",
    "    - 조직 : 조직의 역할과 책임, DBA 관리자\n",
    "    - 프로세스 : 데이터 관리를 위한 활동과 체계\n",
    "\n",
    "\n",
    "7. 데이터 거버넌스 체계\n",
    "    - 데이터 표준화 : 데이터 표준 용어설명, 명명규칙, 사전구축\n",
    "    - 데이터 관리 체계 : 데이터 사전의 관리 원칙 수립\n",
    "    - 데이터 저장소 관리 : 전사 차원의 저장소 구성\n",
    "    - 표준화 활동 : 표준준수 여부를 주기적으로 점검 및 모니터링\n",
    "\n",
    "\n",
    "8. 조직평가를 위한 성숙도 단계\n",
    "    - 도입 단계 : 분석을 시작해 환경을 구축, 일부 조직에서 수행\n",
    "    - 활용 단계 : 전문 담당 부서에서 시행\n",
    "    - 확산 단계 : 전사 모든 부서에서 시행\n",
    "    - 최적화 단계 : 데이터 사이언스 그룹, 경영진 분석 활용\n",
    "\n",
    "\n",
    "9. 개선 방안 수립\n",
    "    - 준비형 : 낮은 준비도 + 낮은 성숙도\n",
    "    - 정착형 : 낮은 준비도 + 높은 성숙도, 정착이 필요한 기업\n",
    "    - 도입형 : 높은 준비도 + 낮은 성숙도, 도입이 필요한 기업\n",
    "    - 확산형 : 높은 준비도 + 높은 성숙도\n",
    "    \n",
    "## 빅데이터 기술 및 제도 \n",
    "\n",
    "### 빅데이터 플랫폼 \n",
    "\n",
    "1. 빅데이터 플랫폼  \n",
    "빅데이터에서 가치를 추출하기 위해 일련의 과정(수집, 저장, 처리, 분석, 시각화)을 규격화한 기술이다.\n",
    "\n",
    "\n",
    "2. 빅데이터 플랫폼 구성요소\n",
    "    - 데이터 수집 : 정형/반정형/비정형 데이터 수집. ETL/크롤러/EAI\n",
    "    - 데이터 저장 : RDBMS, NoSQL\n",
    "    - 데이터 분석 : 텍스트 분석, 머신러닝, 통계, 데이터 마이닝\n",
    "    - 데이터 활용 : 데이터 가시화 및 BI, Open API연계, 히스토그램, 인포그래픽\n",
    "\n",
    "\n",
    "3. 하둡 에코시스템(하둡 프레임워크를 이루고 있는 다양한 서브 프로젝트들의 모임)\n",
    "    - 비정형 데이터 수집\n",
    "        - 척화(Chuckwa) : 분산된 서버에서 에이전트를 실행하고, 컬렉터가 데이터를 받아 HDFS에 저장\n",
    "        - 플럼(Flume) : 로그 데이터를 수집, 이동하기 위해 이벤트와 에이전트를 활용하는 기술\n",
    "        - 스크라이브(Scribe) : 실시간으로 스트리밍 되는 로그데이터를 수집하여 분산 시스템에 데이터를 저장\n",
    "    - 정형 데이터 수집\n",
    "        - 스쿱(Sqoop) : SQL-to-Hadoop. RDBMS에서 HDFS로 데이터를 수집\n",
    "        - 히호(Hiho) : 깃허브에 공개되어 있음. Oracle, MySQL의 데이터만 전송 지원\n",
    "        - 분산 데이터 저장(HDFS) : Hadoop Distributed File System의 약자. 네임노드(파일이름, 권한 등의 속성 기록 및 데이터 노드 위치 파악)와 데이터 노드(일정한 크기로 나눈 블록형태로 저장)로 구성됨.\n",
    "        - 분산 데이터 처리(Map Reduce) : 대용량 데이터 세트를 분산 병렬 컴퓨팅에서 처리하거나 생성하기 위한 목적. 모든 데이터를 Key-value 쌍으로 구성.Map-Shuffle-Reduce 순서대로 데이터를 처리함\n",
    "    - 분산 데이터베이스(Hbase)\n",
    "    - 리소스 관리(YARN)\n",
    "        - 리소스 매니저 : 스케줄러 역할을 수행하고, 클러스터 이용률 최적화를 수행\n",
    "        - 노드 매니저 : 노드 내의 자원 관리\n",
    "        - 애플리케이션 마스터 : 컨테이너를 실행\n",
    "        - 컨테이너 : 가상화 지원\n",
    "    - 인메모리 처리(Aparch Spark) : 하둡 기반 대규모 데이터 분산처리시스템. 스트리밍 데이터, 온라인 머신러닝 등 실시간 데이터 처리\n",
    "    - 데이터 가공\n",
    "        - 피그 : 대용량 데이터 집합을 분석하기 위한 플랫폼. SQL과 유사한 상태로 설계됨\n",
    "        - 하이브 : 하둡 기반의 DW 솔루션. HiveQL이라는 쿼리를 제공\n",
    "        - 데이터 마이닝(Mahout) : 하둡기반으로 데이터 마이닝 알고리즘을 구현한 오픈 소스\n",
    "        - 실시간 SQL 질의(Impala) : 하둡기반의 실시간 SQL질의 시스템. 데이터 조화를 위한 HiveQL을 사용. Hbase와 연동이 가능\n",
    "        - 워크플로우 관리(Oozie) : 하둡 작업을 관리하는 워크플로우 및 코디네이터 시스템\n",
    "        - 분산코디네이션(Zookeeper) : 분산 환경에서 서버들 간에 상호 조정이 필요한 다양한 서비스를 제공. 하나의 서버에만 서비스가 집중되지 않도록 서비스를 알맞게 분산하여 동시에 처리\n",
    "\n",
    "\n",
    "### 인공지능 \n",
    "\n",
    "인공지능 : 인간의 지적능력을 인공적으로 구현하여 컴퓨터가 인간의 지능적인 행동과 사고를 모방할 수 있도록 하는 소프트 웨어\n",
    "\n",
    "### 개인정보보호법 및 제도 \n",
    "\n",
    "1. 개인정보보호  \n",
    "정보 주체(개인)의 개인정보 자기 결정권을 철저히 보장하는 활동\n",
    "\n",
    "\n",
    "2. 개인정보 보호의 필요성 : 유출시 피해 심각, 정보사회 핵심 인프라, 개인정보 자기 통제권\n",
    "\n",
    "\n",
    "3. 개인정보보호법 주요 내용\n",
    "    - 개인정보 수집-이용\n",
    "        - 정보 주체의 동의를 받은 경우\n",
    "        - 법률에 특정한 규정이 있거나, 의무를 준수하기 위해 불가피한 경우\n",
    "        - 공공기관의 소관 업무 수행을 위해 불가피한 경우\n",
    "        - 정보 주체와의 계약 체결을 위하여 필요한 경우\n",
    "        - 정보 주체/범정대리인이 의사표시를 할 수 없는 경우\n",
    "        - 개인정보 처리자의 정당한 이익을 위하여 사용되는 경우\n",
    "    - 개인정보 수집-이용 고지사항\n",
    "        - 목적\n",
    "        - 항목\n",
    "        - 이용기간\n",
    "        - 거부할 권리\n",
    "    - 개인정보 제공\n",
    "        - 개인정보를 제공받는 자\n",
    "        - 목적\n",
    "        - 항목\n",
    "        - 이용기간\n",
    "        - 거부할 권리\n",
    "    - 개인정보 유출시 통지사항\n",
    "        - 개인정보 항목\n",
    "        - 시점과 그 경위\n",
    "        - 정보주체가 할 수 있는 방법\n",
    "        - 대응 조치 및 피해 구제절차\n",
    "        - 담당부서 및 연락처\n",
    "\n",
    "\n",
    "4. 데이터 3법\n",
    "    - 개인정보보호법 : 데이터 이용 활성화를 위한 가명정보 도입. 개인정보 합리화 및 범위 명확화\n",
    "    - 정보통신망법 : '개인정보보호위원회' 변경\n",
    "    - 신용정보법 : 금융 분야 빅데이터 이용의 법적 근거. 마이 데이터 산업 도입. 개인정보보호 강화\n",
    "\n",
    "\n",
    "5. 가명정보\n",
    "    - 개념 : 추가 정보의 사용 없이 특정 개인을 알아볼 수 없게 조치한 정보\n",
    "    - 목적 및 대상\n",
    "        - 통계작성 : 시장 조사\n",
    "        - 과학적 연구\n",
    "        - 공익적 기록 보존\n",
    "\n",
    "\n",
    "6. 프라이버시 보호 모델\n",
    "    - k-익명성 : 주어진 데이터 집합에서 같은 값이 적어도 k개 이상 존재. 연결 공격 취약점 방어\n",
    "    - l-다양성 : 비식별 되는 레코드들은 적어도 l개의 서로 다른 민감한 정보를 가져야 함. 동질성 공격, 배경지식에의한 공격 방어\n",
    "    - t-근접성 : 특정 정보의 분포와 전체 데이터 집합에서 정보의 분포가 t이하의 차이를 보여야 함. 쏠림공격, 유사성 공격 방어\n",
    "    - m-유일성 : 원본 데이터와 동일한 속성 값의 조합이 데이터에 최소 m개 이상 존재.\n",
    "\n",
    "\n",
    "7. 마이 데이터 : 개인은 데이터 주권인 자기 정보 결정권으로 개인 데이터의 활용과 관리에 대한 통제권을 개인이 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd2862",
   "metadata": {},
   "source": [
    "# 데이터 분석 계획 \n",
    "\n",
    "## 분석 방안 수립 \n",
    "\n",
    "### 분석 로드맵 설정 \n",
    "\n",
    "1. 분석 로드맵 단계\n",
    "    - 데이터 분석 체계 도입 : 분석 과제 정의, 분석 기회 발굴, 로드맵 수립\n",
    "    - 데이터 분석 유효성 검증 : 분석 알고리즘 설계, 분석 과제 파일럿 수행\n",
    "    - 데이터 분석 확산 및 고도화 : 시스템 구축, 유관 시스템 고도화\n",
    "\n",
    "### 분석 문제 정의 \n",
    "\n",
    "1. 분석 문제 정의\n",
    "    - 과제 : 처리해야할 문제\n",
    "    - 분석 : 과제와 관련된 현상이나 원인, 해결 방안\n",
    "    - 문제 : 기대 상태와 현재 상태를 동일한 수준으로 맞추는 과정\n",
    "\n",
    "\n",
    "2. 분석 문제 접근 절차\n",
    "    - 하향식 접근 방식 : 분석 과제가 정해져 있는 경우\n",
    "        - 문제 탐색 : 비즈니스 모델 기반 문제 탐색, 분석 기회 발굴의 범위 확장\n",
    "        - 문제 정의 : 사용자 관점에서 비즈니스 문제를 데이터 문제로 변환하여 정의\n",
    "        - 해결방안 탐색 : 분석 기법 및 역량에 따라 다양한 방안으로 탐색\n",
    "        - 타당섬 검토 : 경제성 타당성, 운영적 타당성 검토\n",
    "        - 선택 : 최적 대안 선택\n",
    "    - 상향식 접근 방식 : 문제 정의 자체가 어려운 경우, 디자인 사고 접근법 이용\n",
    "        - 특징\n",
    "            - 비지도 학습 방법 : 데이터 자체의 결합, 연관성, 유사성 등을 중심으로 상태 분석\n",
    "            - 프로토타이핑 접근법 사용 : 시행착오를 통한 문제 해결을 위해 사용. 가설의 생성, 디자인에 대한 실험\n",
    "        - 절차\n",
    "            - 프로세스 분류\n",
    "            - 프로세스 흐름분석\n",
    "            - 분석 요건 식별\n",
    "            - 분석 요건 정의\n",
    "\n",
    "\n",
    "3. 대상별 분석 기획 모형\n",
    "    - 최적화 : 분석대상(Known) + 분석 방법(Known)\n",
    "    - 솔루션 : 분석대상(Known) + 분석 방법(Unknown)\n",
    "    - 통찰 : 분석대상(Unknown) + 분석 방법(Known)\n",
    "    - 발견 : 분석대상(Unknown) + 분석 방법(Unknown)\n",
    "\n",
    "\n",
    "4. 분석 마스터 플랜 수립\n",
    "    - 우선 순위 설정\n",
    "    - 전략적 중요도\n",
    "    - 비즈니스 성과/ROI\n",
    "    - 실행 용이성\n",
    "    - 로드맵 수립\n",
    "    - 업무 내재화 적용 수준\n",
    "    - 분석 데이터 적용 수준\n",
    "    - 기술 적용 수준\n",
    "\n",
    "\n",
    "5. 분석 우선순위 평가\n",
    "    - 시급성 : 전략적 중요도에 부합하는지에 따른 시급성. 현재의 관점에 둘 것인지, 미래의 관점에 둘 것인지를 고려\n",
    "    - 난이도 : 현재 기업의 분석 수준과 데이터를 생성, 저장, 가공 분석하는 비용을 고려한 난이도\n",
    "    - Matrix\n",
    "        - 시급성(현재) + 난이도(쉬움) : 전략적 중요도 높음. 난이도가 쉽고 바로 적용 가능\n",
    "        - 시급성(현재) + 난이도(어려움) : 전략적 중요도 높음. 난이도가 어려움, 바로 적용 불가능\n",
    "        - 시급성(미래) + 난이도(쉬움) : 중장기적 관점 추진. 난이도 쉬움\n",
    "        - 시급성(미래) + 난이도(어려움) : 전략적 중요도가 낮으나 반드시 추진 되어야 함\n",
    "        \n",
    "### 데이터 분석 방안 \n",
    "\n",
    "1. 빅데이터 분석 방법론 계층\n",
    "    - 단계(Phase) : 프로세스 그룹을 통하여 완성된 단계별 산출물이 생성. 기준선으로 설정관리하며, 버전 관리 등을 통한 통제\n",
    "    - 테스크(Task) : 단계를 구성하는 단위 활동\n",
    "    - 스텝(Step) : 입력자료, 처리 및 도구, 출력자료로 구성된 단위 프로세스\n",
    "\n",
    "\n",
    "2. 데이터 분석 방법론의 분석 절차\n",
    "    - 분석 기획 : 비즈니스 이해 및 범위 설정. 프로젝트 정의 및 계획 수립. 프로젝트 위험계획 수립\n",
    "    - 데이터 준비 : 필요 데이터 정의. 데이터 스토어 설계. 데이터 수집 및 정합성 검증\n",
    "    - 데이터 분석 : 분석용 데이터 준비. 텍스트 분석, EDA, 모델링, 모델 평가 및 검증, 모델 적용 및 운영 방안 수립\n",
    "    - 시스템 구현 : 설계 및 구현\n",
    "    - 평가 및 전개 : 모델 발전 계획 수립, 프로젝트 평가 보고\n",
    "\n",
    "\n",
    "3. KDD 분석 방법론\n",
    "    - 데이터 세트 선택 : 도메인에 대한 이해와 목표 데이터 구성 및 생성\n",
    "    - 데이터 전처리 : 노이즈, 결측치, 이상값 제거\n",
    "    - 데이터 변환 : 차원 축소, 변수 변환\n",
    "    - 데이터 마이닝 : 알고리즘 선택, 패턴 찾기, 분류, 예측\n",
    "    - 데이터 마이닝 결과 : 분석 결과에 대한 해석/평가 및 반복 수행\n",
    "\n",
    "\n",
    "4. CRISP-DM 분석 방법론\n",
    "    - 업무 이해 : 비즈니스 이해, 목표 설정, 프로젝트 계획 수립\n",
    "    - 데이터 이해 : 데이터 수집 및 속성 이해, 초기 데이터 수집, 기술 분석, 탐색\n",
    "    - 데이터 준비 : 데이터 정제, 분리, 분석용 데이터 세트 선택\n",
    "    - 모델링 : 알고리즘 선택, 매개변수 최적화\n",
    "    - 평가 : 해석 결과가 프로젝트 목적에 부합하는지 평가\n",
    "    - 전개 : 완성된 모델을 업무에 적용하기 위한 계획 수립\n",
    "\n",
    "\n",
    "5. SEMMA 분석 방법론\n",
    "    - 샘플링 : 분석 데이터 생성\n",
    "    - 탐색 : 기초 통계, 그래픽 탐색, 데이터 오류 검색, 비즈니스 이해\n",
    "    - 수정 : 데이터 수정/변환, 변수 생성, 선택, 변형\n",
    "    - 모델링 : 모델 구축, 패턴 발견, 알고리즘 적용\n",
    "    - 검증 : 모델 평가 검증\n",
    "\n",
    "\n",
    "6. 빅데이터 분석 절차\n",
    "    - 문제 인식\n",
    "    - 연구 조사\n",
    "    - 모형화\n",
    "    - 자료 수집\n",
    "    - 자료 분석\n",
    "    - 분석 결과 공유"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39996608",
   "metadata": {},
   "source": [
    "# 데이터 수집 및 저장 계획 \n",
    "\n",
    "## 데이터 수집 및 전환 \n",
    "\n",
    "### 데이터 수집 \n",
    "\n",
    "1. 수집 데이터의 대상\n",
    "    - 내부 데이터\n",
    "        - 조직(인프라) 내부에 데이터가 위치하며, 데이터 담당자와 수집 주기 및 방법 등을 협의하여 데이터를 수집.\n",
    "        - 수집이 용의한 정형 데이터\n",
    "        - 서비스의 수명 주기 관리가 용이\n",
    "        - 종류\n",
    "            - 서비스 : SCM, ERP, CRM, 포털, 거래 시스템\n",
    "            - 네트워크 : 방화벽, 스위치\n",
    "            - 마케팅 : VOC 접수 데이터, 고객 포털 시스템\n",
    "    - 외부 데이터\n",
    "        - 조직(인프라) 외부에 데이터가 위치하며, 특정 기관의 담당자 협의 또는 데이터 전문 업체를 통해 데이터를 수집\n",
    "        - 공공 데이터 포털을 통해 Open API 또는 파일을 통해 수집\n",
    "        - 주로 수집이 어려운 비정형 데이터\n",
    "        - 종류\n",
    "            - 소셜 : SNS, 커뮤니티, 게시팜\n",
    "            - 네트워크 : 센서 데이터, 장비 간 발생 로그\n",
    "            - 공공 : 공공 데이터(LOD)\n",
    "\n",
    "\n",
    "2. 데이터 수집 방식 및 기술\n",
    "    - ETL(Extract Transform Load) : 데이터를 데이터 저장소인 DW 및 DM으로 이동시키기 위해 다양한 소스 시스템으로부터 필요한 원본 데이터를 추출하고 변환하여 적재하는 작업 및 기술\n",
    "        - 추출 : 이기종 소스 데이터페이스로부터 데이터를 추출. JDBC, ODBC, 3rd Party Tools 활용\n",
    "        - 변환 : 조회 또는 분석을 목적으로 적절한 포맷이나 구조로 데이터를 저장하기 위해 데이터 변환. 데이터 재구성 및 중복 데이터 제거. 일관성 확보를 위한 정제 수행, 데이터 표준화 수행\n",
    "        - 적재 : 데이터를 최종 대상에 저장\n",
    "    - FTP(Flie Transfer Protocol) : TCP/IP 프로토콜을 기반으로 서버, 클라이언트 사이에서 파일 송수신을 하기 위한 프로토콜을 사용한다.\n",
    "        - Active FTP : 클라이언트가 데이터를 수신받을 포트에 서버를 알려주면, 서버가 자신의 20번 포트를 통해 클라이언트의 임의의 포트로 데이터를전송해 주는 방식\n",
    "        - Passive FTP : 서버가 데이터를 송신해줄 임의의 포트를 클라이언트에 알려주면 클라이언트가 서버의 임의의 포트로 접속해서 데이터를 가져가는 방식\n",
    "    - 스쿱(Sqoop) : 커넥터를 사용하여 MySQL 또는 Oracle, 메인 프레임과 같은 관계형 데이터베이스 시스템에서 하둡 파일 시스템으로 데이터를 수집하거나, 하둡 파일 시스템에서 관계형 데이터 베이스로 데이터를 내보낸다.\n",
    "        - 벌크 임푸트 지원 : 전체 데이터베이스 또는 테이블을 HDFS로 한 번에 전송 가능\n",
    "        - 데이터 전송 병렬화 : 시스템 사용률과 성능을 고려한 병렬 데이터 전송\n",
    "        - 직접 입력 자공 : RDB에 매핑해서 Hbase와 Hive에 직접 import제공\n",
    "        - 프로그래밍 방식의 데이터 인터랙션\n",
    "    - 스크래파이(Scrapy) : 파이썬 언어 기반의 비정형 데이터 수집 기술\n",
    "        - 파이썬 기반 : 파이썬 언어 기반으로 구성, 설정이 쉬움\n",
    "        - 단순한 스크랩 과정 : 크롤링 수행 후 바로 데이터 처리 가능\n",
    "        - 다양한 부가 요소 : 쉬운 수집, 로깅 지원\n",
    "    - 아파치 카프카(Apach Kafca) : 대용량 실시간 로그 처리를 위한 분산 스트리밍 플랫폼\n",
    "        - 신뢰성 제공 : 메모리 및 파일 큐 기반의 채널 지원\n",
    "        - 확장성 제공\n",
    "    - 플럼(Flume) : 많은 양의 로그데이터를 효율적으로 수집, 집계, 이동하기 위해 이벤트와 에이전트를 활용하는 기술\n",
    "        - 발행/구독 모델 : 메시지 큐와 유사한 형태의 데이터 큐를 사용, 풀 방식으로 동작하여 부하 감소 및 고성능의 기능 제공\n",
    "        - 고가용성 제공 : 시스템 장애에 대응하여 오랜 기간 동안 지속적인 정상 운영이 가능한 상태 제공\n",
    "        - 파일 기반 저장방식 : 데이터를 순차적으로 저장\n",
    "    - 스크라이브(Scribe)\n",
    "        - 다수의 서버로부터 실시간 스트리밍 되는 로그 데이터를 수집하여 분산 시스템에 데이터를 저장하는 대용량 실시간 로그 수집 기술.\n",
    "        - 단일 중앙 스크라이브 서버와 다수의 로컬 스크라이브 서버로 구성되어 안정성과 확장성을 제공.\n",
    "    - 척와(Chukwa) : 분산된 각 서버에서 에이전트를 실행하고, 컬렉터가 에이전트로부터 데이터를 수집하여 하둡 파일 시스템에 저장, 실시간 분석 기능을 제공하는 기술\n",
    "        - HDFS 연동 : 수집된 로그 파일을 HDFS에 저장하는 기능 지원\n",
    "        - 실시간 분석 제공 : 하둡 HDFS를 통한 실시간 분석 지원\n",
    "        - 청크(Chunk) 단위 처리 : 어댑터가 데이터를 메타데이터가 포함된 청크 단위로 전송\n",
    "    - CEP(Complex Event Processing)\n",
    "        - 여러 이벤트 소스로부터 발생한 이벤트를 실시간으로 추출하여 대응되는 액션을 수행하는 처리기술.\n",
    "        - 실시간 상황에서 의미 있는 이벤트를 파악하고 빨리 대응할 수 있다.\n",
    "    - EAI(Enterprise Application Integration)\n",
    "        - 기업에서 운영되는 서로 다른 플랫폼 및 애플리케이션들 간의 정보 전달, 연계, 통합을 가능하게 해주는 연계기술.\n",
    "        - 각 비즈니스 간 통합 및 연게성을 증대시켜 효율성을 높임.\n",
    "    - CDC(Change Data Capture)\n",
    "        - 최근 변경된 데이터들을 대상으로 다른 시스템으로 이동하는 처리 기술.\n",
    "        - 실시간 백업과 데이터 통합이 가능하여 24시간 운영해야 하는 업무 시스템에 활용\n",
    "    - ODS(Operational Data Store)\n",
    "        - 데이터에 대한 추가 작업을 위해 다양한 데이터 원천 들로부터 데이터를 추출 및 통합한 데이터베이스\n",
    "    - 크롤링(Crawling)\n",
    "        - 인터넷상에서 제공되는 다양한 웹 사이트로부터 소셜 네트워크 정보, 뉴스, 게시판 등의 웹 문서 및 콘텐츠 수집 기술이다.\n",
    "    - RSS(Rich Site Summary)\n",
    "        - 브로그, 뉴스, 쇼핑몰 등의 웹 사이트에 게시된 새로운 글을 공유하기 위해 XML 기바으로 정보를 배포하는 프로토콜을 활용하여 데이터를 수집하는 기술\n",
    "    - Open API\n",
    "        - 응용 프로그램을 통해 실시간으로 데이터를 수신할 수 있도록 공개된 API를 이용하여 데이터를 수집하는 기술\n",
    "    - 스트리밍(Streaming)\n",
    "        - 네트워크를 통해 오디오, 비디오 등의 미디어 데이터를 실시간으로 수집하는 기술    \n",
    "        \n",
    "### 데이터 유형 및 속성 파악 \n",
    "\n",
    "1. 데이터 유형\n",
    "    - 구조 관점의 데이터 유형\n",
    "        - 정형 데이터\n",
    "            - 정형화된 스키마 구조 기반의 형태를 가지고 고정된 필드에 저장되며 값과 형식에서 일관성을 가짐\n",
    "            - 컬럼과 로우 구조를 가지며, 설계된 구조 기반 목적에 맞는 정보들\n",
    "            - RDB, 스프레드시트(Xlsx)\n",
    "        - 반정형 데이터\n",
    "            - 스키마 구조 형태를 가지고 메타데이터를 포함하며 값과 형식에서 일관성을 가지지 않음\n",
    "            - XML, HTML과 같은 Node 형태의 구조를 가짐\n",
    "            - XML, HTML, JSON, RSS\n",
    "        - 비정형 데이터\n",
    "            - 스키마 구조 형태를 가지지 않고 고정된 필드에 저장되지 않음\n",
    "            - text, image, audio, video\n",
    "    - 시간 관점의 데이터 유형\n",
    "        - 실시간 데이터\n",
    "            - 생성된 이후 수 초~수 분 이내에 처리되어야 의미있는 데이터\n",
    "            - 센서 데이터, 시스템 로그, 네트워크 장비 로그, 알람, 보안 장비 로그\n",
    "        - 비 실시간 데이터\n",
    "            - 생성된 데이터가 수 시간 또는 수 주 이후에 처리되어야 의미있는 데이터\n",
    "            - 통계, 웹 로그, 구매 정보\n",
    "    - 저장 형태 관점의 데이터 유형\n",
    "        - 파일 데이터 : 시스템 로그, 서비스 로그 드오가 같이 파일 형식으로 파일 시스템에 저장되는 데이터\n",
    "        - 데이터베이스 데이터 : 관계형 데이터베이스(RDBMS), NoSQL), 인메모리 데이터베이스 등에 의해 칼럼 또는 테이블에 저장된 데이터\n",
    "        - 콘텐츠 데이터 : 미디어 데이터\n",
    "        - 스트림 데이터 : 센서 데이터, HTTP 트랜젝션 등과 같이 네트워크를 통해서 실시간으로 전송되는 데이터\n",
    "\n",
    "\n",
    "2. 데이터 변환기술\n",
    "    - 평활화(Smoothing) : 데이터로부터 잡음을 제거하기 위해 데이터 추세에서 벗어나는 값들을 변환하는 기법, 구간화, 군집화 등의 기법 적용\n",
    "    - 집계(Aggregation) : 다차원의 방법으로 데이터를 요약하는 기법. 복수 개의 속성을 하나로 줄이거나 유사한 데이터 객체를 줄이는 기법\n",
    "    - 일반화(Generalization) : 특정 구간에 분포하는 값으로 스케일을 변화시키는 기법\n",
    "    - 정규화(Normalization) : 데이터를 정해진 구간 내에 들도록 하는 기법. 최소-최대 정규화, Z-score 정규화, 소수 스케일링.\n",
    "    - 속성 생성(Attribute Construction) : 새로운 속성이나 특징을 만드는 방법. 선택한 속성을 하나 이상의 새 속성으로 대체하여 데이터를 변경 처리\n",
    "\n",
    "### 데이터 비식별화\n",
    "\n",
    "1. 데이터 비식별화 처리 기법\n",
    "    - 가명처리 : 개인이 식별 가능한 데이터에 대하여 직접 식별할 수 없는 다른 값으로 대체하는 기법\n",
    "        - 휴라스틱 익명화 : 몇 가지 정해진 규칙을 이용해서 개인정부를 숨기는 방법. 사람의 판단에 따라 가공함\n",
    "        - K-익명화 : 같은 속성값을 가지는 데이터를 K개 이상으로 유지하여 데이터를 공개함\n",
    "        - 암호화 : 일 정 규칙의 알고리즘을 적용하여 암호화함. 암호화/복호화 값(key)를 가지고 있어야 함.\n",
    "        - 교환 방법 : 미리 정해진 변수들의 집합에 대하여 데이터베이스의 레코드와 연계하여 교환\n",
    "    - 총계처리 : 개인저옵에 대하여 통계값을 적용하여 특정 개인을 판단할 수 없도록 하는 기법\n",
    "        - 총계처리 : 데이터 집합 또는 부분으로 집계 처리를 하여 민감성을 낮춤\n",
    "        - 부분집계 : 부분 그룹만 비식별 처리\n",
    "        - 라운딩 : 라운딩(올림, 내림) 기준을 적용하여 최종 집계 처리\n",
    "        - 데이터 재배열 : 기존 정보 값은 유지하면서 개인정보와 연관이 되지 않도록 해당 데이터를 재배열\n",
    "    - 데이터값 삭제 : 개인정보 식별이 가능한 특정 데이터값 삭제 처리 기법\n",
    "        - 속성값 삭제\n",
    "        - 부분 속성값 삭제\n",
    "        - 데이터 행 삭제\n",
    "    - 범주화 : 단일 식별 정보를 해당 그룹의 대푯값으로 변환하거나 구간값으로 변환하는 기법.\n",
    "        - 범주화 : 데이터의 평균 또는 범주의 값으로 변환하는 방식\n",
    "        - 랜덤 올림 방법 : 임의의 수 기준으로 올림 또는 절사 하는 방법\n",
    "    - 데이터 마스킹 : 개인 식별 정보에 대하여 전체 또는 부분적으로 대체값으로 변환. 원시 데이터의 구조에 대한 변형이 적음.\n",
    "\n",
    "\n",
    "2. 개인정보 처리 기법\n",
    "    - 가명 : 개인 식별이 가능한 데이터에 대하여 직접 식별할 수 없는 다른 값으로 대체하는 기법\n",
    "    - 일반화 : 더 일반화된 값으로 대체하는 것으로 숫자데이터의 경우 구간으로 정의하고, 범주화된 속성은 트리의 계층적 구조에 의해 대체하는 기법\n",
    "    - 섭동 : 동일한 확률적 정보를 가지는 변형된 값에 대하여 원래 데이터를 대체하는 기법\n",
    "    - 치환 : 속성 값을 수정하지 않고 레코드 간에 속성값의 위치를 바꾸는 기법\n",
    "    \n",
    "### 데이터 품질 검증 \n",
    "\n",
    "1. 데이터 품질 특성\n",
    "    - 데이터 유효성\n",
    "        - 데이터 정확성\n",
    "            - 정확성 : 실세계에 존재하는 객체의 값이 오류 없이 저장되어 있는 특성\n",
    "            - 사실성 : 데이터가 실세계의 사실과 같은 값을 가지고 있는 특성\n",
    "            - 적합성 : 데이가 정해진 유효 범위를 충족하고 있는 특성\n",
    "            - 필수성 : 필수 항목에 데이터의 누락이 발생하지 않는 특성\n",
    "            - 연관성 : 연관 관계를 가지는 데이터 항목 간에 논리상의 오류가 없는 특성\n",
    "        - 데이터 일관성 :\n",
    "            - 정합성 : 정보시스템 내의 동일한 데이터 간에 불일치가 발생하지 않는 특성\n",
    "            - 일치성 : 상호 동일한 용어와 형태로 정의되어 있는 특성\n",
    "            - 무결성 : 선후 관계가 명확하게 준수되고 있는 특성\n",
    "    - 데이터 활용성\n",
    "        - 데이터 유용성 : 요구사항을 수용할 수 있는 유연한 구조로 되어있는 특성\n",
    "        - 데이터 접근성 : 원하는 데이터를 쉽게 이용할 수 있는 특성\n",
    "        - 데이터 적시성 : 최신성 유지와 같은 품질 요건에 잘 대처되고 있는 특성\n",
    "        - 데이터 보안성 : 보안 + 안정성\n",
    "\n",
    "\n",
    "2. 데이터 품질 검증 프로세스\n",
    "    - 메타데이터를 통한 품질 검증 기법\n",
    "        - 메타 데이터는 데이터에 관한 구조화된 데이터로서 다른 데이터를 설명해 주는 데이터\n",
    "    - 정규 표현식을 통한 검증 기법\n",
    "        - 특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 형식적 언어\n",
    "    - 데이터 프로파일링을 통한 품질 검증 기법\n",
    "        - 데이터 현황 분석을 위한 자료수집을 통해 잠재적 오류 징후를 발견하는 방법. 정의된 표준 도메인에 맞는지 검증.\n",
    "        - 메타데이터 수집 및 분석 -> 대상 및 유형 선정 -> 프로파일링 수행 -> 프로파일링 결과 리뷰 -> 결과 종합\n",
    "        - 검증 특성\n",
    "            - 복잡성 : 데이터의 구조, 형식, 자료, 계층 측면에서 복잡성 기준을 정의\n",
    "            - 완전성 : 수집된 빅데이터 질이 충분하고 완전한지에 대한 품질 관리 기준 정의\n",
    "            - 유용성 : 용이성, 제약 사항 관련 품질 관리 기준 정의\n",
    "            - 시간적 요소 : 자료 수집 및 제공이 주기적으로 가능한지 여부\n",
    "            - 일관성 : 수집된 빅데이터와 원천소스가 연결되지 않는 비율 정도\n",
    "            - 타당성 : 메타데이터를 분석한 방법이 안정성을 평가할 수 있는지 여부\n",
    "            - 정확성 : 자료의 값들이 허용 범위 내에 존재하는지 여부\n",
    "            \n",
    "\n",
    "## 데이터 적재 및 저장 \n",
    "\n",
    "### 데이터 적재 \n",
    "\n",
    "1. 데이터 적재\n",
    "    - 데이터를 수집한 후에는 수집한 데이터를 빅데이터 시스템에 적재해야 한다.\n",
    "    - 적재할 빅데이터의 유형과 실시간 처리 여부에 따라 RDBMS, HDFS, NoSQL 저장 시스템에 적재할 수 있다.\n",
    "\n",
    "\n",
    "2. 데이터 적재 도구\n",
    "    - 플루언티드(Fluented) : 트레저 데이터에서 개발된 크로스 플랫폼 오픈 소스 데이터. 루비 프로그래밍 언어로 작성됨.\n",
    "    - 플럼(Flume) : 많은 양의 로그 데이터를 효율적으로 수집, 집계 및 이동하기 위해 이벤트와 에이전트를 활용하는 분산형 로그 수집 기술.\n",
    "    - 스크라이브(Scribe) : 다수의 서버로부터 실시간으로 스트리밍되는 로그 데이터를 수집하여 분산 시스템에 데이터를 저장하는 대용량 실시간 로그 수집 기술.\n",
    "    - 로그스태시(Logstash) : 모든 로그 정보를 수집하여 하나의 DB에 출력해 주는 시스템\n",
    "    \n",
    "### 데이터 저장 \n",
    "\n",
    "1. 데이터 저장 기술\n",
    "    - 데이터 웨어하우스\n",
    "        - 사용자의 의사결정에 도움을 주기 위하여, 기간 시스템의 데이터베이스에 축적된 데이터를 공통 형식으로 변환해서 관리하는 데이터 베이스.\n",
    "        - 고도로 정제된 데이터로 스키마가 정의되어야 저장할 수 있다.\n",
    "        - 주제 지향적 : 기능이나 업무가 아닌 주제 중심적으로 구성됨\n",
    "        - 통합적 : 데이터의 일관성을 유지하면서 전사적 관점에서 하나로 통함됨\n",
    "        - 시 계열적 : 시간에 따른 변경을 항상 반영하고 있음\n",
    "        - 비휘발적 : 적재가 완료되면 읽기전용 형태의 스냅 샷 형태로 존재\n",
    "    - 데이터 마트\n",
    "        - 전사적으로 구축된 데이터 속의 특정 주제, 부서 중심으로 구축된 소규모 단위 주제의 데이터 웨어하우스.\n",
    "        - DW 환경에서 정의된 접근계층으로, 데이터 웨어하우스에서 데이터를 꺼내 사용자에게 제공하는 역할\n",
    "        - 데이터 웨어하우스의 부분이며, 대개 특정한 조직 혹은 팀에서 사용하는 것을 목적으로 함\n",
    "    - 데이터 레이크\n",
    "        - 정형, 반정형, 비정형 데이터를 비롯한 모든 가공되지 않은 Raw Data를 저장할 수 있는 시스템 또는 중앙 집중식 데이터 저장소\n",
    "        - 구조화된 데이터 : RDBMS, 반구조화된 데이터 : CSV, XML, JSON\n",
    "\n",
    "\n",
    "2. 빅데이터 저장기술 : 분산 파일 시스템\n",
    "    - 개념 : 컴퓨터 네트워크를 통해 공유하는 여러 호스트 컴퓨터의 파일에 접근할 수 있게 하는 파일 시스템\n",
    "    - 구글 파일 시스템(GFS)\n",
    "        - 구글의 대규모 클러스터 서비스 플랫폼의 기반이 되는 파일 시스템\n",
    "        - 파일을 고정된 크기(64MB)의 청크들로 나누며 복제본을 청크 서버에 분산하여 저장한다.\n",
    "        - 구성요소\n",
    "            - 클라이언트 : 파일에 대한 읽기/쓰기 동작을 요청하는 애플리케이션으로 POSIX 인터페이스를 지원하지 않음. 여러 클라이언트에서 원자적인\n",
    "            - 데이터 추가 연산을 지원하기 위한 인터페이스 지원.\n",
    "            - 마스터 : 단일 마스터 구조로 파일 시스템의 이름 공간, 파일과 청크의 매핑 정보, 각 청크가 저장된 청크 서버들의 위치 정보 등에 해당하는 모든 메타데이터를 메모리상에서 관리. 주기적으로 청크 서버의 하트비트 메시지를 이용하여 청크를 재복제하거나 재 분산하여 상태를 관리\n",
    "            - 청크서버 : 로컬 디스크에 청크를 저장. 클라이언트가 청크 입출력을 요청하면 청크 서버가 처리.\n",
    "    - 하둡 분산 파일 시스템(HDFS)\n",
    "        - 대용량 파일을 분산된 서버에 저장하고, 저장된 데이터를 빠르게 처리할 수 있게 하는 분산 파일 시스템\n",
    "        - 저사양의 다수 서브를 이용해서 스토리지를 구성할 수 있어 기존의 대용량 파일 시스템에 비해 비용관점에서 효율적이다.\n",
    "        - 구성요소\n",
    "        네임노드 : HDFS 상의 모든 메타데이터를 관리하며 마스터/슬레이브 구조에서 마스터 역할 수행\n",
    "        보조네임 노드 : HDFS 상태 모니터링을 보조. 주기적으로 네임 노드의 파일 시스템 이미지를 스냇샵으로 생성\n",
    "        데이터 노드 : HDFS의 슬레이브 노드로, 데이터 입출력 요청을 처리\n",
    "    - 러스터\n",
    "        - 클러스터 파일 시스템에서 개발한 객체 기반의 클러스터 파일 시스템\n",
    "        - 구성요소\n",
    "            - 클라이언트 파일 시스템 : 메타데이터 서버와 객체 저장 서버들과 통신하면서 클라이언트 응용에 파일 시스템 인터페이스를 제공\n",
    "            - 메타데이터 서버 : 파일 시스템의 이름 공간과 파일에 대한 메타데이터를 관리\n",
    "            - 객체 저장 서버 : 파일 데이터를 저장하고, 클라이언트로부터의 객체 입출력 요청을 처리.\n",
    "\n",
    "\n",
    "3. 빅데이터 저장기술 : 데이터베이스 클러스터\n",
    "    - 개념 : 관계형 데이터베이스 관리 시스템으로 하나의 데이터베이스를 여러 개의 서버상에 구축하는 시스템\n",
    "    - 구분\n",
    "        - 공유 디스크 클러스터 : 데이터 파일은 논리적으로 데이터 파일을 공유하여 모든 데이터에 접근 가능하게 하는 방식. 높은 수준의 고가용성을 제공하므로 클러스터 노드 중 하나만 살아 있어도 서비스가 가능.\n",
    "        - 무공유 디스크 클러스터 : 데이터베이스 인스턴스는 자신이 관리하는 데이터 파일을 자신의 로컬 디스크에 저장하며, 이 파일들은 노드 간에 공유하지 않음. 노드 확장에 제한은 없지만, 각 노드에 장애가 발생할 경우 FTA를 구성해야함.\n",
    "    - 종류\n",
    "        - Oracle RAC : 공유 클러스터, 고가용성과 확장이 쉬운 장점이 있음.\n",
    "        - IBM DB2 ICE : 무공유 클러스터\n",
    "        - SQL Server : 연합 데이터베이스 형태로 여러 노드로 확장할 수 있는 기능 제공.\n",
    "        - MySQL : 비 공유형, 메모리 기반 데이터베이스의 클러스터링 지원.\n",
    "\n",
    "\n",
    "2. 빅데이터 저장기술 : NoSQL\n",
    "    - 개념 : 전통적인 RDBMS와 다른 DBMS를 지칭하기 위한 용어로 데이터 저장에 고정된 테이블 스키마가 필요하지 않고 조인 연산을 사용할 수 없으며, 수평적으로 확장이 가능한 DBMS.\n",
    "    - 특성\n",
    "        - Basically Available : 언제든지 접근할 수 있는 속성\n",
    "        - oft-Stable : 노드의 상태는 외부에서 전송된 정보를 통해 결정되는 속성\n",
    "        - Eventually Consistency : 일정 시간이 지나면 데이터의 일관성이 유지되는 속성\n",
    "    - 유형\n",
    "        - Key-Value Store : Unique 한 Key에 하나의 Value를 가지고 있는 형태\n",
    "        - Column Family Data Store : Key 안에 조합으로 된 여러 개의 필드를 갖는 DB\n",
    "        - Document Store : Value의 데이터 타입이 Document라는 타입을 사용하는 DB. XML, JSON, YAML과 같이 구조화된 데이터 타입으로 계층 구조 표현.\n",
    "        - Graph Store : 시멘틱 웹과 온톨로지 분야에서 활용되는 그래프로 데이터를 표현하는 DB\n",
    "    - CAP이론\n",
    "        - 일관성(Consisntence) : 모든 사용자에게 같은 시간에는 같은 데이터를 보여주어야 한다는 특성\n",
    "        - 유효성(Availibility) : 모든 클라이언트가 읽기 및 쓰기가 가능해야 한다는 특성. 하나의 노드에 장애가 일어나더라도 다른 노드에는 영향을 미치면 안 되는 특성\n",
    "        - 분산 가능(Partition Tolerance) : 물리적 네트워크 분산 환경에서 시스템이 원할하게 동작해야 한다는 특성\n",
    "    - 종류\n",
    "        - 구글 빅테이블 : 구글 클라우드 플랫폼에서 데이터 저장소로 사용.\n",
    "        - Hbase : HDFS를 기반으로 구현된 컬럼 기반의 분산 데이터 베이스. 비 관계형으로 SQL을 지원하지 않으며 수평적으로 확장이 있음.\n",
    "        - Amazon SimpleDB : Amazon의 데이터 서비스 플랫폼으로, 웹 에플리케이션에서 사용하는 데이터 실시간 처리를 지원\n",
    "        - Microsoft SSDS : 테이블과 유사한 컨테이너, 레코드와 유사한 엔티티로 구성. 컨테이너의 생성/삭제, 엔티티의 생성/삭제, 조회, 쿼리 등의 API를 제공함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
